{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "## DATASET \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you're running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "# Network Parameters\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WEIGHTS AND BIASES\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convoloutions\n",
    "![Alt Text](./convolution-schematic.gif \"Convolutions\")\n",
    "Convolution with 3Ã—3 Filter. \n",
    "Source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution\n",
    "\n",
    "The above is an example of a convolution with a 3x3 filter and a stride of 1 being applied to data with a range of 0 to 1. The convolution for each 3x3 section is calculated against the weight, `[[1, 0, 1], [0, 1, 0], [1, 0, 1]]`, then a bias is added to create the convolved feature on the right. In this case, the bias is zero. In TensorFlow, this is all done using `tf.nn.conv2d()` and `tf.nn.bias_add()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```tf.nn.conv2d()``` function computes the convolution against weight W as shown above.\n",
    "\n",
    "In TensorFlow, ```strides``` is an array of 4 elements; the first element in this array indicates the stride for batch and last element indicates stride for features. It's good practice to remove the batches or features you want to skip from the data set rather than use a stride to skip them. You can always set the first and last element to 1 in ```strides``` in order to use all batches and features.\n",
    "\n",
    "The middle two elements are the strides for height and width respectively. I've mentioned stride as one number because you usually have a square stride where ```height = width```. When someone says they are using a stride of 3, they usually mean ```tf.nn.conv2d(x, W, strides=[1, 3, 3, 1])```.\n",
    "\n",
    "To make life easier, the code is using ```tf.nn.bias_add()``` to add the bias. Using ```tf.add()``` doesn't work when the tensors aren't the same shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling\n",
    "![Alt Text](./maxpool.jpeg \"Convolutions\")\n",
    "Max Pooling with 2x2 filter and stride of 2. Source: http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "The above is an example of a convolution with a 3x3 filter and a stride of 1 being applied to data with a range of 0 to 1. The convolution for each 3x3 section is calculated against the weight, `[[1, 0, 1], [0, 1, 0], [1, 0, 1]]`, then a bias is added to create the convolved feature on the right. In this case, the bias is zero. In TensorFlow, this is all done using `tf.nn.conv2d()` and `tf.nn.bias_add()`.\n",
    "\n",
    "\n",
    "The above is an example of max pooling with a 2x2 filter and stride of 2. The left square is the input and the right square is the output. The four 2x2 colors in input represents each time the filter was applied to create the max on the right side. For example, ```[[1, 1], [5, 6]]``` becomes 6 and ```[[3, 2], [1, 2]]``` becomes 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```tf.nn.max_pool()``` function does exactly what you would expect, it performs max pooling with the ```ksize``` parameter as the size of the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "![Alt Text](./model.png \"Model\")\n",
    "Image from Explore The Design Space video\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the code below, we're creating 3 layers alternating between convolutions and max pooling followed by a fully connected and output layer. The transformation of each layer to new dimensions are shown in the comments. For example, the first layer shapes the images from `28x28x1` to `28x28x32` in the convolution step. Then next step applies max pooling, turning each sample into `14x14x32`. All the layers are applied from `conv1` to `output`, producing `10 class predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch   1 -Loss: 58806.8242 Validation Accuracy: 0.183594\n",
      "Epoch  1, Batch   2 -Loss: 61161.5625 Validation Accuracy: 0.207031\n",
      "Epoch  1, Batch   3 -Loss: 44938.5156 Validation Accuracy: 0.207031\n",
      "Epoch  1, Batch   4 -Loss: 39770.7812 Validation Accuracy: 0.203125\n",
      "Epoch  1, Batch   5 -Loss: 37001.0977 Validation Accuracy: 0.214844\n",
      "Epoch  1, Batch   6 -Loss: 39916.2578 Validation Accuracy: 0.207031\n",
      "Epoch  1, Batch   7 -Loss: 33631.2930 Validation Accuracy: 0.230469\n",
      "Epoch  1, Batch   8 -Loss: 36265.4844 Validation Accuracy: 0.214844\n",
      "Epoch  1, Batch   9 -Loss: 28241.0195 Validation Accuracy: 0.234375\n",
      "Epoch  1, Batch  10 -Loss: 32385.5410 Validation Accuracy: 0.253906\n",
      "Epoch  1, Batch  11 -Loss: 24401.3633 Validation Accuracy: 0.257812\n",
      "Epoch  1, Batch  12 -Loss: 27961.1465 Validation Accuracy: 0.253906\n",
      "Epoch  1, Batch  13 -Loss: 18316.3438 Validation Accuracy: 0.269531\n",
      "Epoch  1, Batch  14 -Loss: 22705.5742 Validation Accuracy: 0.261719\n",
      "Epoch  1, Batch  15 -Loss: 22872.4570 Validation Accuracy: 0.253906\n",
      "Epoch  1, Batch  16 -Loss: 18761.1445 Validation Accuracy: 0.257812\n",
      "Epoch  1, Batch  17 -Loss: 22992.4844 Validation Accuracy: 0.269531\n",
      "Epoch  1, Batch  18 -Loss: 17345.2617 Validation Accuracy: 0.257812\n",
      "Epoch  1, Batch  19 -Loss: 20386.7812 Validation Accuracy: 0.261719\n",
      "Epoch  1, Batch  20 -Loss: 18326.9414 Validation Accuracy: 0.281250\n",
      "Epoch  1, Batch  21 -Loss: 16908.7305 Validation Accuracy: 0.285156\n",
      "Epoch  1, Batch  22 -Loss: 15728.4375 Validation Accuracy: 0.300781\n",
      "Epoch  1, Batch  23 -Loss: 12867.4043 Validation Accuracy: 0.308594\n",
      "Epoch  1, Batch  24 -Loss: 13153.8564 Validation Accuracy: 0.308594\n",
      "Epoch  1, Batch  25 -Loss: 14033.9521 Validation Accuracy: 0.324219\n",
      "Epoch  1, Batch  26 -Loss: 10408.0586 Validation Accuracy: 0.335938\n",
      "Epoch  1, Batch  27 -Loss: 13951.6641 Validation Accuracy: 0.343750\n",
      "Epoch  1, Batch  28 -Loss: 12671.6738 Validation Accuracy: 0.351562\n",
      "Epoch  1, Batch  29 -Loss: 11066.0234 Validation Accuracy: 0.375000\n",
      "Epoch  1, Batch  30 -Loss: 10434.5137 Validation Accuracy: 0.386719\n",
      "Epoch  1, Batch  31 -Loss: 10551.0918 Validation Accuracy: 0.375000\n",
      "Epoch  1, Batch  32 -Loss: 11729.6953 Validation Accuracy: 0.390625\n",
      "Epoch  1, Batch  33 -Loss: 10503.6631 Validation Accuracy: 0.390625\n",
      "Epoch  1, Batch  34 -Loss: 12753.1133 Validation Accuracy: 0.402344\n",
      "Epoch  1, Batch  35 -Loss: 11880.3701 Validation Accuracy: 0.410156\n",
      "Epoch  1, Batch  36 -Loss:  9481.5303 Validation Accuracy: 0.414062\n",
      "Epoch  1, Batch  37 -Loss:  8984.2715 Validation Accuracy: 0.437500\n",
      "Epoch  1, Batch  38 -Loss: 10151.2852 Validation Accuracy: 0.429688\n",
      "Epoch  1, Batch  39 -Loss:  9740.0137 Validation Accuracy: 0.453125\n",
      "Epoch  1, Batch  40 -Loss: 12740.8506 Validation Accuracy: 0.417969\n",
      "Epoch  1, Batch  41 -Loss: 11026.5576 Validation Accuracy: 0.433594\n",
      "Epoch  1, Batch  42 -Loss:  7919.5874 Validation Accuracy: 0.437500\n",
      "Epoch  1, Batch  43 -Loss: 10940.6152 Validation Accuracy: 0.457031\n",
      "Epoch  1, Batch  44 -Loss:  8745.8574 Validation Accuracy: 0.457031\n",
      "Epoch  1, Batch  45 -Loss:  8693.0156 Validation Accuracy: 0.464844\n",
      "Epoch  1, Batch  46 -Loss:  6603.5195 Validation Accuracy: 0.476562\n",
      "Epoch  1, Batch  47 -Loss:  7272.9585 Validation Accuracy: 0.476562\n",
      "Epoch  1, Batch  48 -Loss:  7622.9482 Validation Accuracy: 0.500000\n",
      "Epoch  1, Batch  49 -Loss:  8937.7676 Validation Accuracy: 0.480469\n",
      "Epoch  1, Batch  50 -Loss:  8283.6660 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch  51 -Loss:  8397.4590 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch  52 -Loss:  8941.4805 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch  53 -Loss:  9012.0811 Validation Accuracy: 0.527344\n",
      "Epoch  1, Batch  54 -Loss:  7012.4297 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch  55 -Loss:  7228.5264 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch  56 -Loss:  6232.7393 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch  57 -Loss:  6191.6333 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch  58 -Loss:  6746.5435 Validation Accuracy: 0.527344\n",
      "Epoch  1, Batch  59 -Loss:  7824.8701 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch  60 -Loss:  6435.1758 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch  61 -Loss:  8500.8721 Validation Accuracy: 0.550781\n",
      "Epoch  1, Batch  62 -Loss:  7468.5479 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch  63 -Loss:  7561.6372 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch  64 -Loss:  6665.0732 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch  65 -Loss:  8375.1611 Validation Accuracy: 0.570312\n",
      "Epoch  1, Batch  66 -Loss:  9974.1074 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch  67 -Loss:  5965.8628 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch  68 -Loss:  6148.0708 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch  69 -Loss:  7338.5952 Validation Accuracy: 0.582031\n",
      "Epoch  1, Batch  70 -Loss:  6443.1221 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch  71 -Loss:  7733.7349 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch  72 -Loss:  5066.5059 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch  73 -Loss:  6471.2266 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch  74 -Loss:  6022.1719 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch  75 -Loss:  5037.2686 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch  76 -Loss:  4477.3740 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch  77 -Loss:  5739.6016 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch  78 -Loss:  6883.3076 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch  79 -Loss:  6103.5752 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch  80 -Loss:  5777.8125 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch  81 -Loss:  6072.5908 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch  82 -Loss:  5607.8589 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch  83 -Loss:  5892.9268 Validation Accuracy: 0.597656\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2d77cac3cf50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 keep_prob: dropout})\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# Calculate batch loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## SESSION\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf. global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: 1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: mnist.validation.images[:test_valid_size],\n",
    "                y: mnist.validation.labels[:test_valid_size],\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: mnist.test.images[:test_valid_size],\n",
    "        y: mnist.test.labels[:test_valid_size],\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
