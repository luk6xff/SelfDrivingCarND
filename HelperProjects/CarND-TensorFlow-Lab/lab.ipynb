{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">TensorFlow Neural Network Lab</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/notmnist.png\">\n",
    "In this lab, you'll use all the tools you learned from *Introduction to TensorFlow* to label images of English letters! The data you are using, <a href=\"http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html\">notMNIST</a>, consists of images of a letter from A to J in differents font.\n",
    "\n",
    "The above images are a few examples of the data you'll be training on. After training the network, you will compare your prediction model against test data. Your goal, by the end of this lab, is to make predictions against that test set with at least an 80% accuracy. Let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start this lab, you first need to import all the necessary modules. Run the code below. If it runs successfully, it will print \"`All modules imported`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notMNIST dataset is too large for many computers to handle.  It contains 500,000 images for just training.  You'll be using a subset of this data, 15,000 images for each label (A-J)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading notMNIST_train.zip...\n",
      "Download Finished\n",
      "Downloading notMNIST_test.zip...\n",
      "Download Finished\n",
      "All files downloaded.\n"
     ]
    }
   ],
   "source": [
    "def download(url, file):\n",
    "    \"\"\"\n",
    "    Download file from <url>\n",
    "    :param url: URL to file\n",
    "    :param file: Local file path\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        print('Downloading ' + file + '...')\n",
    "        urlretrieve(url, file)\n",
    "        print('Download Finished')\n",
    "\n",
    "# Download the training and test dataset.\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_train.zip', 'notMNIST_train.zip')\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_test.zip', 'notMNIST_test.zip')\n",
    "\n",
    "# Make sure the files aren't corrupted\n",
    "assert hashlib.md5(open('notMNIST_train.zip', 'rb').read()).hexdigest() == 'c8673b3f28f489e9cdf3a3d74e2ac8fa',\\\n",
    "        'notMNIST_train.zip file is corrupted.  Remove the file and try again.'\n",
    "assert hashlib.md5(open('notMNIST_test.zip', 'rb').read()).hexdigest() == '5d3c7e653e63471c88df796156a9dfa9',\\\n",
    "        'notMNIST_test.zip file is corrupted.  Remove the file and try again.'\n",
    "\n",
    "# Wait until you see that all files have been downloaded.\n",
    "print('All files downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/210001 [00:00<?, ?files/s]\u001b[A\n",
      "  0%|          | 674/210001 [00:00<00:31, 6731.84files/s]\u001b[A\n",
      "  1%|          | 1338/210001 [00:00<00:31, 6703.72files/s]\u001b[A\n",
      "  1%|          | 1995/210001 [00:00<00:31, 6661.57files/s]\u001b[A\n",
      "  1%|▏         | 2661/210001 [00:00<00:31, 6658.76files/s]\u001b[A\n",
      "  2%|▏         | 3330/210001 [00:00<00:31, 6666.37files/s]\u001b[A\n",
      "  2%|▏         | 3976/210001 [00:00<00:31, 6602.59files/s]\u001b[A\n",
      "  2%|▏         | 4660/210001 [00:00<00:30, 6669.79files/s]\u001b[A\n",
      "  3%|▎         | 5339/210001 [00:00<00:30, 6704.03files/s]\u001b[A\n",
      "  3%|▎         | 5977/210001 [00:00<00:30, 6602.24files/s]\u001b[A\n",
      "  3%|▎         | 6616/210001 [00:01<00:31, 6536.90files/s]\u001b[A\n",
      "  3%|▎         | 7272/210001 [00:01<00:30, 6542.81files/s]\u001b[A\n",
      "  4%|▍         | 7919/210001 [00:01<00:31, 6517.02files/s]\u001b[A\n",
      "  4%|▍         | 8612/210001 [00:01<00:30, 6635.39files/s]\u001b[A\n",
      "  4%|▍         | 9290/210001 [00:01<00:30, 6677.09files/s]\u001b[A\n",
      "  5%|▍         | 9954/210001 [00:01<00:30, 6579.22files/s]\u001b[A\n",
      "  5%|▌         | 10629/210001 [00:01<00:30, 6629.34files/s]\u001b[A\n",
      "  5%|▌         | 11291/210001 [00:01<00:30, 6621.31files/s]\u001b[A\n",
      "  6%|▌         | 11952/210001 [00:01<00:30, 6584.80files/s]\u001b[A\n",
      "  6%|▌         | 12610/210001 [00:01<00:30, 6571.00files/s]\u001b[A\n",
      "  6%|▋         | 13267/210001 [00:02<00:30, 6501.56files/s]\u001b[A\n",
      "  7%|▋         | 13946/210001 [00:02<00:29, 6583.58files/s]\u001b[A\n",
      "  7%|▋         | 14605/210001 [00:02<00:29, 6540.54files/s]\u001b[A\n",
      "  7%|▋         | 15260/210001 [00:02<00:29, 6529.46files/s]\u001b[A\n",
      "  8%|▊         | 15923/210001 [00:02<00:29, 6559.05files/s]\u001b[A\n",
      "  8%|▊         | 16597/210001 [00:02<00:29, 6610.99files/s]\u001b[A\n",
      "  8%|▊         | 17271/210001 [00:02<00:28, 6646.79files/s]\u001b[A\n",
      "  9%|▊         | 17950/210001 [00:02<00:28, 6687.78files/s]\u001b[A\n",
      "  9%|▉         | 18621/210001 [00:02<00:28, 6693.73files/s]\u001b[A\n",
      "  9%|▉         | 19298/210001 [00:02<00:28, 6715.04files/s]\u001b[A\n",
      " 10%|▉         | 19971/210001 [00:03<00:28, 6719.40files/s]\u001b[A\n",
      " 10%|▉         | 20644/210001 [00:03<00:28, 6589.65files/s]\u001b[A\n",
      " 10%|█         | 21304/210001 [00:03<00:30, 6211.46files/s]\u001b[A\n",
      " 10%|█         | 21931/210001 [00:03<00:30, 6200.25files/s]\u001b[A\n",
      " 11%|█         | 22555/210001 [00:03<00:30, 6176.88files/s]\u001b[A\n",
      " 11%|█         | 23176/210001 [00:03<00:31, 6025.78files/s]\u001b[A\n",
      " 11%|█▏        | 23782/210001 [00:03<00:32, 5712.38files/s]\u001b[A\n",
      " 12%|█▏        | 24359/210001 [00:03<00:33, 5596.20files/s]\u001b[A\n",
      " 12%|█▏        | 24985/210001 [00:03<00:32, 5778.36files/s]\u001b[A\n",
      " 12%|█▏        | 25593/210001 [00:03<00:31, 5864.46files/s]\u001b[A\n",
      " 12%|█▏        | 26198/210001 [00:04<00:31, 5917.70files/s]\u001b[A\n",
      " 13%|█▎        | 26793/210001 [00:04<00:31, 5897.63files/s]\u001b[A\n",
      " 13%|█▎        | 27414/210001 [00:04<00:30, 5987.90files/s]\u001b[A\n",
      " 13%|█▎        | 28039/210001 [00:04<00:30, 6062.13files/s]\u001b[A\n",
      " 14%|█▎        | 28647/210001 [00:04<00:30, 5978.33files/s]\u001b[A\n",
      " 14%|█▍        | 29247/210001 [00:04<00:30, 5943.78files/s]\u001b[A\n",
      " 14%|█▍        | 29874/210001 [00:04<00:29, 6037.70files/s]\u001b[A\n",
      " 15%|█▍        | 30538/210001 [00:04<00:28, 6205.00files/s]\u001b[A\n",
      " 15%|█▍        | 31184/210001 [00:04<00:28, 6278.04files/s]\u001b[A\n",
      " 15%|█▌        | 31814/210001 [00:04<00:28, 6228.64files/s]\u001b[A\n",
      " 15%|█▌        | 32440/210001 [00:05<00:28, 6236.41files/s]\u001b[A\n",
      " 16%|█▌        | 33100/210001 [00:05<00:27, 6339.33files/s]\u001b[A\n",
      " 16%|█▌        | 33756/210001 [00:05<00:27, 6402.21files/s]\u001b[A\n",
      " 16%|█▋        | 34403/210001 [00:05<00:27, 6420.19files/s]\u001b[A\n",
      " 17%|█▋        | 35066/210001 [00:05<00:26, 6481.41files/s]\u001b[A\n",
      " 17%|█▋        | 35715/210001 [00:05<00:27, 6441.32files/s]\u001b[A\n",
      " 17%|█▋        | 36360/210001 [00:05<00:27, 6322.41files/s]\u001b[A\n",
      " 18%|█▊        | 36994/210001 [00:05<00:27, 6212.23files/s]\u001b[A\n",
      " 18%|█▊        | 37617/210001 [00:05<00:28, 6054.96files/s]\u001b[A\n",
      " 18%|█▊        | 38225/210001 [00:06<00:29, 5911.57files/s]\u001b[A\n",
      " 18%|█▊        | 38845/210001 [00:06<00:28, 5994.52files/s]\u001b[A\n",
      " 19%|█▉        | 39505/210001 [00:06<00:27, 6162.96files/s]\u001b[A\n",
      " 19%|█▉        | 40126/210001 [00:06<00:27, 6175.95files/s]\u001b[A\n",
      " 19%|█▉        | 40746/210001 [00:06<00:27, 6182.19files/s]\u001b[A\n",
      " 20%|█▉        | 41366/210001 [00:06<00:27, 6108.01files/s]\u001b[A\n",
      " 20%|█▉        | 41987/210001 [00:06<00:27, 6137.84files/s]\u001b[A\n",
      " 20%|██        | 42606/210001 [00:06<00:27, 6152.53files/s]\u001b[A\n",
      " 21%|██        | 43228/210001 [00:06<00:27, 6172.31files/s]\u001b[A\n",
      " 21%|██        | 43884/210001 [00:06<00:26, 6281.99files/s]\u001b[A\n",
      " 21%|██        | 44513/210001 [00:07<00:26, 6248.79files/s]\u001b[A\n",
      " 21%|██▏       | 45139/210001 [00:07<00:26, 6183.77files/s]\u001b[A\n",
      " 22%|██▏       | 45798/210001 [00:07<00:26, 6297.72files/s]\u001b[A\n",
      " 22%|██▏       | 46458/210001 [00:07<00:25, 6384.20files/s]\u001b[A\n",
      " 22%|██▏       | 47116/210001 [00:07<00:25, 6440.59files/s]\u001b[A\n",
      " 23%|██▎       | 47761/210001 [00:07<00:25, 6441.01files/s]\u001b[A\n",
      " 23%|██▎       | 48408/210001 [00:07<00:25, 6447.72files/s]\u001b[A\n",
      " 23%|██▎       | 49054/210001 [00:07<00:24, 6449.58files/s]\u001b[A\n",
      " 24%|██▎       | 49726/210001 [00:07<00:24, 6526.67files/s]\u001b[A\n",
      " 24%|██▍       | 50381/210001 [00:07<00:24, 6532.37files/s]\u001b[A\n",
      " 24%|██▍       | 51041/210001 [00:08<00:24, 6550.02files/s]\u001b[A\n",
      " 25%|██▍       | 51697/210001 [00:08<00:24, 6549.53files/s]\u001b[A\n",
      " 25%|██▍       | 52353/210001 [00:08<00:24, 6482.23files/s]\u001b[A\n",
      " 25%|██▌       | 53002/210001 [00:08<00:25, 6239.80files/s]\u001b[A\n",
      " 26%|██▌       | 53629/210001 [00:08<00:25, 6080.29files/s]\u001b[A\n",
      " 26%|██▌       | 54240/210001 [00:08<00:25, 6076.45files/s]\u001b[A\n",
      " 26%|██▌       | 54863/210001 [00:08<00:25, 6120.86files/s]\u001b[A\n",
      " 26%|██▋       | 55477/210001 [00:08<00:25, 6022.76files/s]\u001b[A\n",
      " 27%|██▋       | 56106/210001 [00:08<00:25, 6099.62files/s]\u001b[A\n",
      " 27%|██▋       | 56743/210001 [00:08<00:24, 6177.93files/s]\u001b[A\n",
      " 27%|██▋       | 57379/210001 [00:09<00:24, 6229.53files/s]\u001b[A\n",
      " 28%|██▊       | 58032/210001 [00:09<00:24, 6316.22files/s]\u001b[A\n",
      " 28%|██▊       | 58686/210001 [00:09<00:23, 6381.36files/s]\u001b[A\n",
      " 28%|██▊       | 59330/210001 [00:09<00:23, 6398.00files/s]\u001b[A\n",
      " 29%|██▊       | 59971/210001 [00:09<00:23, 6401.57files/s]\u001b[A\n",
      " 29%|██▉       | 60612/210001 [00:09<00:23, 6401.86files/s]\u001b[A\n",
      " 29%|██▉       | 61256/210001 [00:09<00:23, 6411.91files/s]\u001b[A\n",
      " 29%|██▉       | 61901/210001 [00:09<00:23, 6422.90files/s]\u001b[A\n",
      " 30%|██▉       | 62544/210001 [00:09<00:23, 6400.66files/s]\u001b[A\n",
      " 30%|███       | 63187/210001 [00:09<00:22, 6408.47files/s]\u001b[A\n",
      " 30%|███       | 63828/210001 [00:10<00:22, 6400.81files/s]\u001b[A\n",
      " 31%|███       | 64469/210001 [00:10<00:22, 6383.44files/s]\u001b[A\n",
      " 31%|███       | 65108/210001 [00:10<00:22, 6375.11files/s]\u001b[A\n",
      " 31%|███▏      | 65755/210001 [00:10<00:22, 6402.55files/s]\u001b[A\n",
      " 32%|███▏      | 66396/210001 [00:10<00:22, 6278.87files/s]\u001b[A\n",
      " 32%|███▏      | 67025/210001 [00:10<00:22, 6251.59files/s]\u001b[A\n",
      " 32%|███▏      | 67651/210001 [00:10<00:22, 6234.00files/s]\u001b[A\n",
      " 33%|███▎      | 68275/210001 [00:10<00:23, 6122.10files/s]\u001b[A\n",
      " 33%|███▎      | 68935/210001 [00:10<00:22, 6257.19files/s]\u001b[A\n",
      " 33%|███▎      | 69595/210001 [00:10<00:22, 6354.04files/s]\u001b[A\n",
      " 33%|███▎      | 70232/210001 [00:11<00:22, 6343.21files/s]\u001b[A\n",
      " 34%|███▎      | 70868/210001 [00:11<00:22, 6271.23files/s]\u001b[A\n",
      " 34%|███▍      | 71497/210001 [00:11<00:22, 6274.56files/s]\u001b[A\n",
      " 34%|███▍      | 72126/210001 [00:11<00:22, 6101.52files/s]\u001b[A\n",
      " 35%|███▍      | 72738/210001 [00:11<00:22, 6038.67files/s]\u001b[A\n",
      " 35%|███▍      | 73344/210001 [00:11<00:22, 5992.91files/s]\u001b[A\n",
      " 35%|███▌      | 73950/210001 [00:11<00:22, 6012.57files/s]\u001b[A\n",
      " 36%|███▌      | 74580/210001 [00:11<00:22, 6095.46files/s]\u001b[A\n",
      " 36%|███▌      | 75199/210001 [00:11<00:22, 6122.87files/s]\u001b[A\n",
      " 36%|███▌      | 75835/210001 [00:12<00:21, 6189.57files/s]\u001b[A\n",
      " 36%|███▋      | 76455/210001 [00:12<00:22, 6056.47files/s]\u001b[A\n",
      " 37%|███▋      | 77062/210001 [00:12<00:22, 6007.33files/s]\u001b[A\n",
      " 37%|███▋      | 77667/210001 [00:12<00:21, 6019.35files/s]\u001b[A\n",
      " 37%|███▋      | 78318/210001 [00:12<00:21, 6156.83files/s]\u001b[A\n",
      " 38%|███▊      | 78974/210001 [00:12<00:20, 6272.12files/s]\u001b[A\n",
      " 38%|███▊      | 79603/210001 [00:12<00:20, 6219.66files/s]\u001b[A\n",
      " 38%|███▊      | 80246/210001 [00:12<00:20, 6278.74files/s]\u001b[A\n",
      " 39%|███▊      | 80875/210001 [00:12<00:20, 6209.74files/s]\u001b[A\n",
      " 39%|███▉      | 81497/210001 [00:12<00:21, 5965.92files/s]\u001b[A\n",
      " 39%|███▉      | 82109/210001 [00:13<00:21, 6009.92files/s]\u001b[A\n",
      " 39%|███▉      | 82712/210001 [00:13<00:21, 5943.44files/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 83308/210001 [00:13<00:21, 5908.83files/s]\u001b[A\n",
      " 40%|███▉      | 83900/210001 [00:13<00:21, 5781.87files/s]\u001b[A\n",
      " 40%|████      | 84480/210001 [00:13<00:21, 5771.96files/s]\u001b[A\n",
      " 41%|████      | 85073/210001 [00:13<00:21, 5816.72files/s]\u001b[A\n",
      " 41%|████      | 85689/210001 [00:13<00:21, 5915.54files/s]\u001b[A\n",
      " 41%|████      | 86310/210001 [00:13<00:20, 6000.64files/s]\u001b[A\n",
      " 41%|████▏     | 86924/210001 [00:13<00:20, 6040.31files/s]\u001b[A\n",
      " 42%|████▏     | 87573/210001 [00:13<00:19, 6166.53files/s]\u001b[A\n",
      " 42%|████▏     | 88201/210001 [00:14<00:19, 6198.89files/s]\u001b[A\n",
      " 42%|████▏     | 88825/210001 [00:14<00:19, 6208.24files/s]\u001b[A\n",
      " 43%|████▎     | 89447/210001 [00:14<00:19, 6203.93files/s]\u001b[A\n",
      " 43%|████▎     | 90074/210001 [00:14<00:19, 6222.03files/s]\u001b[A\n",
      " 43%|████▎     | 90697/210001 [00:14<00:19, 6207.76files/s]\u001b[A\n",
      " 44%|████▎     | 91362/210001 [00:14<00:18, 6332.26files/s]\u001b[A\n",
      " 44%|████▍     | 91996/210001 [00:14<00:19, 6197.81files/s]\u001b[A\n",
      " 44%|████▍     | 92644/210001 [00:14<00:18, 6277.41files/s]\u001b[A\n",
      " 44%|████▍     | 93304/210001 [00:14<00:18, 6370.13files/s]\u001b[A\n",
      " 45%|████▍     | 93967/210001 [00:14<00:18, 6444.27files/s]\u001b[A\n",
      " 45%|████▌     | 94640/210001 [00:15<00:17, 6525.65files/s]\u001b[A\n",
      " 45%|████▌     | 95326/210001 [00:15<00:17, 6622.08files/s]\u001b[A\n",
      " 46%|████▌     | 96008/210001 [00:15<00:17, 6677.75files/s]\u001b[A\n",
      " 46%|████▌     | 96692/210001 [00:15<00:16, 6724.29files/s]\u001b[A\n",
      " 46%|████▋     | 97366/210001 [00:15<00:16, 6710.66files/s]\u001b[A\n",
      " 47%|████▋     | 98038/210001 [00:15<00:16, 6659.21files/s]\u001b[A\n",
      " 47%|████▋     | 98724/210001 [00:15<00:16, 6717.24files/s]\u001b[A\n",
      " 47%|████▋     | 99408/210001 [00:15<00:16, 6752.89files/s]\u001b[A\n",
      " 48%|████▊     | 100084/210001 [00:15<00:16, 6747.66files/s]\u001b[A\n",
      " 48%|████▊     | 100759/210001 [00:15<00:16, 6689.40files/s]\u001b[A\n",
      " 48%|████▊     | 101438/210001 [00:16<00:16, 6719.08files/s]\u001b[A\n",
      " 49%|████▊     | 102121/210001 [00:16<00:15, 6750.77files/s]\u001b[A\n",
      " 49%|████▉     | 102797/210001 [00:16<00:16, 6687.00files/s]\u001b[A\n",
      " 49%|████▉     | 103466/210001 [00:16<00:15, 6662.80files/s]\u001b[A\n",
      " 50%|████▉     | 104142/210001 [00:16<00:15, 6689.73files/s]\u001b[A\n",
      " 50%|████▉     | 104812/210001 [00:16<00:15, 6670.93files/s]\u001b[A\n",
      " 50%|█████     | 105480/210001 [00:16<00:15, 6599.17files/s]\u001b[A\n",
      " 51%|█████     | 106166/210001 [00:16<00:15, 6673.45files/s]\u001b[A\n",
      " 51%|█████     | 106857/210001 [00:16<00:15, 6742.48files/s]\u001b[A\n",
      " 51%|█████     | 107532/210001 [00:16<00:15, 6614.57files/s]\u001b[A\n",
      " 52%|█████▏    | 108195/210001 [00:17<00:15, 6363.15files/s]\u001b[A\n",
      " 52%|█████▏    | 108834/210001 [00:17<00:16, 6241.85files/s]\u001b[A\n",
      " 52%|█████▏    | 109516/210001 [00:17<00:15, 6404.28files/s]\u001b[A\n",
      " 52%|█████▏    | 110200/210001 [00:17<00:15, 6528.83files/s]\u001b[A\n",
      " 53%|█████▎    | 110856/210001 [00:17<00:15, 6525.76files/s]\u001b[A\n",
      " 53%|█████▎    | 111511/210001 [00:17<00:15, 6517.84files/s]\u001b[A\n",
      " 53%|█████▎    | 112188/210001 [00:17<00:14, 6587.07files/s]\u001b[A\n",
      " 54%|█████▎    | 112848/210001 [00:17<00:14, 6504.89files/s]\u001b[A\n",
      " 54%|█████▍    | 113500/210001 [00:17<00:14, 6476.74files/s]\u001b[A\n",
      " 54%|█████▍    | 114174/210001 [00:18<00:14, 6552.16files/s]\u001b[A\n",
      " 55%|█████▍    | 114831/210001 [00:18<00:14, 6555.55files/s]\u001b[A\n",
      " 55%|█████▍    | 115500/210001 [00:18<00:14, 6593.04files/s]\u001b[A\n",
      " 55%|█████▌    | 116187/210001 [00:18<00:14, 6671.84files/s]\u001b[A\n",
      " 56%|█████▌    | 116873/210001 [00:18<00:13, 6725.43files/s]\u001b[A\n",
      " 56%|█████▌    | 117546/210001 [00:18<00:13, 6699.31files/s]\u001b[A\n",
      " 56%|█████▋    | 118223/210001 [00:18<00:13, 6719.77files/s]\u001b[A\n",
      " 57%|█████▋    | 118896/210001 [00:18<00:14, 6492.39files/s]\u001b[A\n",
      " 57%|█████▋    | 119548/210001 [00:18<00:14, 6235.01files/s]\u001b[A\n",
      " 57%|█████▋    | 120175/210001 [00:18<00:14, 6138.91files/s]\u001b[A\n",
      " 58%|█████▊    | 120792/210001 [00:19<00:14, 6040.08files/s]\u001b[A\n",
      " 58%|█████▊    | 121455/210001 [00:19<00:14, 6203.88files/s]\u001b[A\n",
      " 58%|█████▊    | 122125/210001 [00:19<00:13, 6343.50files/s]\u001b[A\n",
      " 58%|█████▊    | 122784/210001 [00:19<00:13, 6413.51files/s]\u001b[A\n",
      " 59%|█████▉    | 123447/210001 [00:19<00:13, 6476.74files/s]\u001b[A\n",
      " 59%|█████▉    | 124097/210001 [00:19<00:13, 6355.70files/s]\u001b[A\n",
      " 59%|█████▉    | 124754/210001 [00:19<00:13, 6416.70files/s]\u001b[A\n",
      " 60%|█████▉    | 125430/210001 [00:19<00:12, 6514.77files/s]\u001b[A\n",
      " 60%|██████    | 126083/210001 [00:19<00:12, 6489.75files/s]\u001b[A\n",
      " 60%|██████    | 126736/210001 [00:19<00:12, 6500.42files/s]\u001b[A\n",
      " 61%|██████    | 127389/210001 [00:20<00:12, 6508.20files/s]\u001b[A\n",
      " 61%|██████    | 128041/210001 [00:20<00:12, 6497.43files/s]\u001b[A\n",
      " 61%|██████▏   | 128692/210001 [00:20<00:12, 6461.85files/s]\u001b[A\n",
      " 62%|██████▏   | 129359/210001 [00:20<00:12, 6521.47files/s]\u001b[A\n",
      " 62%|██████▏   | 130018/210001 [00:20<00:12, 6539.93files/s]\u001b[A\n",
      " 62%|██████▏   | 130673/210001 [00:20<00:12, 6538.08files/s]\u001b[A\n",
      " 63%|██████▎   | 131328/210001 [00:20<00:12, 6540.68files/s]\u001b[A\n",
      " 63%|██████▎   | 131983/210001 [00:20<00:12, 6415.78files/s]\u001b[A\n",
      " 63%|██████▎   | 132626/210001 [00:20<00:12, 6306.39files/s]\u001b[A\n",
      " 63%|██████▎   | 133258/210001 [00:20<00:12, 6211.89files/s]\u001b[A\n",
      " 64%|██████▍   | 133886/210001 [00:21<00:12, 6228.00files/s]\u001b[A\n",
      " 64%|██████▍   | 134510/210001 [00:21<00:12, 6118.15files/s]\u001b[A\n",
      " 64%|██████▍   | 135123/210001 [00:21<00:12, 5944.33files/s]\u001b[A\n",
      " 65%|██████▍   | 135771/210001 [00:21<00:12, 6094.60files/s]\u001b[A\n",
      " 65%|██████▍   | 136438/210001 [00:21<00:11, 6256.44files/s]\u001b[A\n",
      " 65%|██████▌   | 137067/210001 [00:21<00:11, 6244.82files/s]\u001b[A\n",
      " 66%|██████▌   | 137694/210001 [00:21<00:11, 6181.70files/s]\u001b[A\n",
      " 66%|██████▌   | 138314/210001 [00:21<00:11, 6009.20files/s]\u001b[A\n",
      " 66%|██████▌   | 138917/210001 [00:21<00:12, 5901.44files/s]\u001b[A\n",
      " 66%|██████▋   | 139571/210001 [00:22<00:11, 6077.49files/s]\u001b[A\n",
      " 67%|██████▋   | 140185/210001 [00:22<00:11, 6094.72files/s]\u001b[A\n",
      " 67%|██████▋   | 140797/210001 [00:22<00:11, 6049.44files/s]\u001b[A\n",
      " 67%|██████▋   | 141433/210001 [00:22<00:11, 6139.30files/s]\u001b[A\n",
      " 68%|██████▊   | 142107/210001 [00:22<00:10, 6306.81files/s]\u001b[A\n",
      " 68%|██████▊   | 142760/210001 [00:22<00:10, 6369.96files/s]\u001b[A\n",
      " 68%|██████▊   | 143399/210001 [00:22<00:10, 6181.03files/s]\u001b[A\n",
      " 69%|██████▊   | 144020/210001 [00:22<00:10, 6150.86files/s]\u001b[A\n",
      " 69%|██████▉   | 144669/210001 [00:22<00:10, 6246.60files/s]\u001b[A\n",
      " 69%|██████▉   | 145296/210001 [00:22<00:10, 6135.50files/s]\u001b[A\n",
      " 69%|██████▉   | 145918/210001 [00:23<00:10, 6159.63files/s]\u001b[A\n",
      " 70%|██████▉   | 146536/210001 [00:23<00:10, 6107.00files/s]\u001b[A\n",
      " 70%|███████   | 147148/210001 [00:23<00:10, 5771.89files/s]\u001b[A\n",
      " 70%|███████   | 147848/210001 [00:23<00:10, 6092.14files/s]\u001b[A\n",
      " 71%|███████   | 148541/210001 [00:23<00:09, 6319.81files/s]\u001b[A\n",
      " 71%|███████   | 149218/210001 [00:23<00:09, 6445.88files/s]\u001b[A\n",
      " 71%|███████▏  | 149909/210001 [00:23<00:09, 6577.56files/s]\u001b[A\n",
      " 72%|███████▏  | 150573/210001 [00:23<00:09, 6594.28files/s]\u001b[A\n",
      " 72%|███████▏  | 151236/210001 [00:23<00:09, 6340.72files/s]\u001b[A\n",
      " 72%|███████▏  | 151875/210001 [00:24<00:09, 5972.89files/s]\u001b[A\n",
      " 73%|███████▎  | 152502/210001 [00:24<00:09, 6058.27files/s]\u001b[A\n",
      " 73%|███████▎  | 153175/210001 [00:24<00:09, 6244.36files/s]\u001b[A\n",
      " 73%|███████▎  | 153858/210001 [00:24<00:08, 6406.61files/s]\u001b[A\n",
      " 74%|███████▎  | 154504/210001 [00:24<00:08, 6336.47files/s]\u001b[A\n",
      " 74%|███████▍  | 155142/210001 [00:24<00:08, 6323.80files/s]\u001b[A\n",
      " 74%|███████▍  | 155808/210001 [00:24<00:08, 6418.46files/s]\u001b[A\n",
      " 75%|███████▍  | 156487/210001 [00:24<00:08, 6523.11files/s]\u001b[A\n",
      " 75%|███████▍  | 157188/210001 [00:24<00:07, 6659.53files/s]\u001b[A\n",
      " 75%|███████▌  | 157879/210001 [00:24<00:07, 6732.21files/s]\u001b[A\n",
      " 76%|███████▌  | 158554/210001 [00:25<00:07, 6726.80files/s]\u001b[A\n",
      " 76%|███████▌  | 159228/210001 [00:25<00:07, 6545.28files/s]\u001b[A\n",
      " 76%|███████▌  | 159885/210001 [00:25<00:07, 6340.90files/s]\u001b[A\n",
      " 76%|███████▋  | 160522/210001 [00:25<00:07, 6270.88files/s]\u001b[A\n",
      " 77%|███████▋  | 161152/210001 [00:25<00:07, 6233.60files/s]\u001b[A\n",
      " 77%|███████▋  | 161782/210001 [00:25<00:07, 6251.23files/s]\u001b[A\n",
      " 77%|███████▋  | 162409/210001 [00:25<00:07, 6212.67files/s]\u001b[A\n",
      " 78%|███████▊  | 163056/210001 [00:25<00:07, 6287.57files/s]\u001b[A\n",
      " 78%|███████▊  | 163686/210001 [00:25<00:07, 6274.32files/s]\u001b[A\n",
      " 78%|███████▊  | 164323/210001 [00:25<00:07, 6299.90files/s]\u001b[A\n",
      " 79%|███████▊  | 164962/210001 [00:26<00:07, 6325.64files/s]\u001b[A\n",
      " 79%|███████▉  | 165664/210001 [00:26<00:06, 6517.56files/s]\u001b[A\n",
      " 79%|███████▉  | 166318/210001 [00:26<00:06, 6486.32files/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 166968/210001 [00:26<00:06, 6376.50files/s]\u001b[A\n",
      " 80%|███████▉  | 167633/210001 [00:26<00:06, 6455.58files/s]\u001b[A\n",
      " 80%|████████  | 168349/210001 [00:26<00:06, 6650.07files/s]\u001b[A\n",
      " 81%|████████  | 169056/210001 [00:26<00:06, 6768.62files/s]\u001b[A\n",
      " 81%|████████  | 169735/210001 [00:26<00:05, 6723.53files/s]\u001b[A\n",
      " 81%|████████  | 170409/210001 [00:26<00:05, 6656.69files/s]\u001b[A\n",
      " 81%|████████▏ | 171076/210001 [00:26<00:05, 6567.19files/s]\u001b[A\n",
      " 82%|████████▏ | 171802/210001 [00:27<00:05, 6760.47files/s]\u001b[A\n",
      " 82%|████████▏ | 172530/210001 [00:27<00:05, 6908.31files/s]\u001b[A\n",
      " 82%|████████▏ | 173246/210001 [00:27<00:05, 6979.83files/s]\u001b[A\n",
      " 83%|████████▎ | 173968/210001 [00:27<00:05, 7049.46files/s]\u001b[A\n",
      " 83%|████████▎ | 174701/210001 [00:27<00:04, 7130.77files/s]\u001b[A\n",
      " 84%|████████▎ | 175437/210001 [00:27<00:04, 7196.23files/s]\u001b[A\n",
      " 84%|████████▍ | 176158/210001 [00:27<00:04, 7096.29files/s]\u001b[A\n",
      " 84%|████████▍ | 176869/210001 [00:27<00:04, 6821.21files/s]\u001b[A\n",
      " 85%|████████▍ | 177555/210001 [00:27<00:04, 6650.98files/s]\u001b[A\n",
      " 85%|████████▍ | 178224/210001 [00:27<00:04, 6660.69files/s]\u001b[A\n",
      " 85%|████████▌ | 178893/210001 [00:28<00:04, 6574.86files/s]\u001b[A\n",
      " 86%|████████▌ | 179572/210001 [00:28<00:04, 6637.04files/s]\u001b[A\n",
      " 86%|████████▌ | 180248/210001 [00:28<00:04, 6672.89files/s]\u001b[A\n",
      " 86%|████████▌ | 180917/210001 [00:28<00:04, 6523.20files/s]\u001b[A\n",
      " 86%|████████▋ | 181571/210001 [00:28<00:04, 6435.59files/s]\u001b[A\n",
      " 87%|████████▋ | 182216/210001 [00:28<00:04, 6410.31files/s]\u001b[A\n",
      " 87%|████████▋ | 182904/210001 [00:28<00:04, 6542.69files/s]\u001b[A\n",
      " 87%|████████▋ | 183630/210001 [00:28<00:03, 6741.34files/s]\u001b[A\n",
      " 88%|████████▊ | 184355/210001 [00:28<00:03, 6884.22files/s]\u001b[A\n",
      " 88%|████████▊ | 185062/210001 [00:29<00:03, 6937.08files/s]\u001b[A\n",
      " 88%|████████▊ | 185758/210001 [00:29<00:03, 6711.01files/s]\u001b[A\n",
      " 89%|████████▉ | 186432/210001 [00:29<00:03, 6652.05files/s]\u001b[A\n",
      " 89%|████████▉ | 187110/210001 [00:29<00:03, 6687.31files/s]\u001b[A\n",
      " 89%|████████▉ | 187836/210001 [00:29<00:03, 6848.38files/s]\u001b[A\n",
      " 90%|████████▉ | 188552/210001 [00:29<00:03, 6938.31files/s]\u001b[A\n",
      " 90%|█████████ | 189270/210001 [00:29<00:02, 7007.13files/s]\u001b[A\n",
      " 90%|█████████ | 189984/210001 [00:29<00:02, 7043.71files/s]\u001b[A\n",
      " 91%|█████████ | 190696/210001 [00:29<00:02, 7065.29files/s]\u001b[A\n",
      " 91%|█████████ | 191412/210001 [00:29<00:02, 7092.69files/s]\u001b[A\n",
      " 91%|█████████▏| 192122/210001 [00:30<00:02, 7067.18files/s]\u001b[A\n",
      " 92%|█████████▏| 192834/210001 [00:30<00:02, 7080.32files/s]\u001b[A\n",
      " 92%|█████████▏| 193543/210001 [00:30<00:02, 7057.05files/s]\u001b[A\n",
      " 92%|█████████▏| 194249/210001 [00:30<00:02, 7044.09files/s]\u001b[A\n",
      " 93%|█████████▎| 194968/210001 [00:30<00:02, 7084.14files/s]\u001b[A\n",
      " 93%|█████████▎| 195694/210001 [00:30<00:02, 7134.60files/s]\u001b[A\n",
      " 94%|█████████▎| 196414/210001 [00:30<00:01, 7152.42files/s]\u001b[A\n",
      " 94%|█████████▍| 197130/210001 [00:30<00:01, 7141.55files/s]\u001b[A\n",
      " 94%|█████████▍| 197845/210001 [00:30<00:01, 7047.50files/s]\u001b[A\n",
      " 95%|█████████▍| 198551/210001 [00:30<00:01, 6980.55files/s]\u001b[A\n",
      " 95%|█████████▍| 199250/210001 [00:31<00:01, 6968.45files/s]\u001b[A\n",
      " 95%|█████████▌| 199948/210001 [00:31<00:01, 6894.17files/s]\u001b[A\n",
      " 96%|█████████▌| 200638/210001 [00:31<00:01, 6828.25files/s]\u001b[A\n",
      " 96%|█████████▌| 201336/210001 [00:31<00:01, 6871.12files/s]\u001b[A\n",
      " 96%|█████████▌| 202024/210001 [00:31<00:01, 6867.96files/s]\u001b[A\n",
      " 97%|█████████▋| 202715/210001 [00:31<00:01, 6878.81files/s]\u001b[A\n",
      " 97%|█████████▋| 203408/210001 [00:31<00:00, 6892.95files/s]\u001b[A\n",
      " 97%|█████████▋| 204098/210001 [00:31<00:00, 6822.19files/s]\u001b[A\n",
      " 98%|█████████▊| 204781/210001 [00:31<00:00, 6402.93files/s]\u001b[A\n",
      " 98%|█████████▊| 205427/210001 [00:31<00:00, 6019.84files/s]\u001b[A\n",
      " 98%|█████████▊| 206038/210001 [00:32<00:00, 5980.63files/s]\u001b[A\n",
      " 98%|█████████▊| 206687/210001 [00:32<00:00, 6122.93files/s]\u001b[A\n",
      " 99%|█████████▊| 207346/210001 [00:32<00:00, 6253.69files/s]\u001b[A\n",
      " 99%|█████████▉| 207987/210001 [00:32<00:00, 6299.30files/s]\u001b[A\n",
      " 99%|█████████▉| 208628/210001 [00:32<00:00, 6329.41files/s]\u001b[A\n",
      "100%|█████████▉| 209289/210001 [00:32<00:00, 6409.52files/s]\u001b[A\n",
      "100%|█████████▉| 209932/210001 [00:32<00:00, 6215.65files/s]\u001b[A\n",
      "100%|██████████| 210001/210001 [00:32<00:00, 6416.24files/s]\u001b[A\n",
      "  0%|          | 0/10001 [00:00<?, ?files/s]\u001b[A\n",
      "  7%|▋         | 679/10001 [00:00<00:01, 6788.27files/s]\u001b[A\n",
      " 14%|█▎        | 1367/10001 [00:00<00:01, 6813.69files/s]\u001b[A\n",
      " 20%|██        | 2030/10001 [00:00<00:01, 6756.03files/s]\u001b[A\n",
      " 27%|██▋       | 2735/10001 [00:00<00:01, 6839.76files/s]\u001b[A\n",
      " 34%|███▍      | 3434/10001 [00:00<00:00, 6882.70files/s]\u001b[A\n",
      " 41%|████▏     | 4144/10001 [00:00<00:00, 6944.00files/s]\u001b[A\n",
      " 48%|████▊     | 4841/10001 [00:00<00:00, 6950.45files/s]\u001b[A\n",
      " 56%|█████▌    | 5553/10001 [00:00<00:00, 6998.34files/s]\u001b[A\n",
      " 63%|██████▎   | 6263/10001 [00:00<00:00, 7025.91files/s]\u001b[A\n",
      " 70%|██████▉   | 6975/10001 [00:01<00:00, 7052.80files/s]\u001b[A\n",
      " 77%|███████▋  | 7696/10001 [00:01<00:00, 7097.81files/s]\u001b[A\n",
      " 84%|████████▍ | 8436/10001 [00:01<00:00, 7184.97files/s]\u001b[A\n",
      " 92%|█████████▏| 9163/10001 [00:01<00:00, 7210.24files/s]\u001b[A\n",
      " 99%|█████████▉| 9878/10001 [00:01<00:00, 7122.07files/s]\u001b[A\n",
      "100%|██████████| 10001/10001 [00:01<00:00, 7019.03files/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features and labels uncompressed.\n"
     ]
    }
   ],
   "source": [
    "def uncompress_features_labels(file):\n",
    "    \"\"\"\n",
    "    Uncompress features and labels from a zip file\n",
    "    :param file: The zip file to extract the data from\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with ZipFile(file) as zipf:\n",
    "        # Progress Bar\n",
    "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
    "        \n",
    "        # Get features and labels from all files\n",
    "        for filename in filenames_pbar:\n",
    "            # Check if the file is a directory\n",
    "            if not filename.endswith('/'):\n",
    "                with zipf.open(filename) as image_file:\n",
    "                    image = Image.open(image_file)\n",
    "                    image.load()\n",
    "                    # Load image data as 1 dimensional array\n",
    "                    # We're using float32 to save on memory space\n",
    "                    feature = np.array(image, dtype=np.float32).flatten()\n",
    "\n",
    "                # Get the the letter from the filename.  This is the letter of the image.\n",
    "                label = os.path.split(filename)[1][0]\n",
    "\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Get the features and labels from the zip files\n",
    "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
    "test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')\n",
    "\n",
    "# Limit the amount of data to work with a docker container\n",
    "docker_size_limit = 150000\n",
    "train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)\n",
    "\n",
    "# Set flags for feature engineering.  This will prevent you from skipping an important step.\n",
    "is_features_normal = False\n",
    "is_labels_encod = False\n",
    "\n",
    "# Wait until you see that all features and labels have been uncompressed.\n",
    "print('All features and labels uncompressed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/mean_variance.png\" style=\"height: 75%;width: 75%; position: relative; right: 5%\">\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "The first problem involves normalizing the features for your training and test data.\n",
    "\n",
    "Implement Min-Max scaling in the `normalize()` function to a range of `a=0.1` and `b=0.9`. After scaling, the values of the pixels in the input data should range from 0.1 to 0.9.\n",
    "\n",
    "Since the raw notMNIST image data is in [grayscale](https://en.wikipedia.org/wiki/Grayscale), the current values range from a min of 0 to a max of 255.\n",
    "\n",
    "Min-Max Scaling:\n",
    "$\n",
    "X'=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}\n",
    "$\n",
    "\n",
    "*If you're having trouble solving problem 1, you can view the solution [here](https://github.com/udacity/CarND-TensorFlow-Lab/blob/master/solutions.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "# Problem 1 - Implement Min-Max scaling for grayscale image data\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Min-Max scaling for grayscale image data\n",
    "    a = 0.1\n",
    "    b = 0.9\n",
    "    grayscale_max = 255\n",
    "    grayscale_min = 0\n",
    "    return a + (((image_data - grayscale_min)*(b-a)) / (grayscale_max - grayscale_min))\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# Test Cases\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize_grayscale(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 255])),\n",
    "    [0.1, 0.103137254902, 0.106274509804, 0.109411764706, 0.112549019608, 0.11568627451, 0.118823529412, 0.121960784314,\n",
    "     0.125098039216, 0.128235294118, 0.13137254902, 0.9],\n",
    "    decimal=3)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize_grayscale(np.array([0, 1, 10, 20, 30, 40, 233, 244, 254,255])),\n",
    "    [0.1, 0.103137254902, 0.13137254902, 0.162745098039, 0.194117647059, 0.225490196078, 0.830980392157, 0.865490196078,\n",
    "     0.896862745098, 0.9])\n",
    "\n",
    "if not is_features_normal:\n",
    "    train_features = normalize_grayscale(train_features)\n",
    "    test_features = normalize_grayscale(test_features)\n",
    "    is_features_normal = True\n",
    "\n",
    "print('Tests Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels One-Hot Encoded\n"
     ]
    }
   ],
   "source": [
    "if not is_labels_encod:\n",
    "    # Turn labels into numbers and apply One-Hot Encoding\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(train_labels)\n",
    "    train_labels = encoder.transform(train_labels)\n",
    "    test_labels = encoder.transform(test_labels)\n",
    "\n",
    "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "    train_labels = train_labels.astype(np.float32)\n",
    "    test_labels = test_labels.astype(np.float32)\n",
    "    is_labels_encod = True\n",
    "\n",
    "print('Labels One-Hot Encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features and labels randomized and split.\n"
     ]
    }
   ],
   "source": [
    "assert is_features_normal, 'You skipped the step to normalize the features'\n",
    "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\n",
    "\n",
    "# Get randomized datasets for training and validation\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_size=0.05,\n",
    "    random_state=832289)\n",
    "\n",
    "print('Training features and labels randomized and split.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to pickle file...\n",
      "Data cached in pickle file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data for easy access\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open('notMNIST.pickle', 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'train_dataset': train_features,\n",
    "                    'train_labels': train_labels,\n",
    "                    'valid_dataset': valid_features,\n",
    "                    'valid_labels': valid_labels,\n",
    "                    'test_dataset': test_features,\n",
    "                    'test_labels': test_labels,\n",
    "                },\n",
    "                pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "print('Data cached in pickle file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "All your progress is now saved to the pickle file.  If you need to leave and comeback to this lab, you no longer have to start from the beginning.  Just run the code block below and it will load all the data and modules required to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and modules loaded.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Load the modules\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload the data\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  pickle_data = pickle.load(f)\n",
    "  train_features = pickle_data['train_dataset']\n",
    "  train_labels = pickle_data['train_labels']\n",
    "  valid_features = pickle_data['valid_dataset']\n",
    "  valid_labels = pickle_data['valid_labels']\n",
    "  test_features = pickle_data['test_dataset']\n",
    "  test_labels = pickle_data['test_labels']\n",
    "  del pickle_data  # Free up memory\n",
    "\n",
    "\n",
    "print('Data and modules loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/weight_biases.png\" style=\"height: 60%;width: 60%; position: relative; right: 10%\">\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "For the neural network to train on your data, you need the following <a href=\"https://www.tensorflow.org/api_docs/python/tf/dtypes/DType\">float32</a> tensors:\n",
    " - `features`\n",
    "  - Placeholder tensor for feature data (`train_features`/`valid_features`/`test_features`)\n",
    " - `labels`\n",
    "  - Placeholder tensor for label data (`train_labels`/`valid_labels`/`test_labels`)\n",
    " - `weights`\n",
    "  - Variable Tensor with random numbers from a truncated normal distribution.\n",
    "    - See <a href=\"https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal\">`tf.truncated_normal()` documentation</a> for help.\n",
    " - `biases`\n",
    "  - Variable Tensor with all zeros.\n",
    "    - See <a href=\"https://www.tensorflow.org/api_docs/python/tf/zeros\"> `tf.zeros()` documentation</a> for help.\n",
    "\n",
    "*If you're having trouble solving problem 2, review \"TensorFlow Linear Function\" section of the class.  If that doesn't help, the solution for this problem is available [here](https://github.com/udacity/CarND-TensorFlow-Lab/blob/master/solutions.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "features_count = 784 # MNIST data input (img shape: 28*28)\n",
    "labels_count = 10    # MNIST total classes (0-9 digits)\n",
    "\n",
    "# TODO: Set the features and labels tensors\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# TODO: Set the weights and biases tensors\n",
    "weights = tf.Variable(tf.truncated_normal([features_count, labels_count]))\n",
    "biases =  tf.Variable(tf.zeros(labels_count))\n",
    "\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "\n",
    "#Test Cases\n",
    "from tensorflow.python.ops.variables import Variable\n",
    "\n",
    "assert features._op.name.startswith('Placeholder'), 'features must be a placeholder'\n",
    "assert labels._op.name.startswith('Placeholder'), 'labels must be a placeholder'\n",
    "assert isinstance(weights, Variable), 'weights must be a TensorFlow variable'\n",
    "assert isinstance(biases, Variable), 'biases must be a TensorFlow variable'\n",
    "\n",
    "assert features._shape == None or (\\\n",
    "    features._shape.dims[0].value is None and\\\n",
    "    features._shape.dims[1].value in [None, 784]), 'The shape of features is incorrect'\n",
    "assert labels._shape  == None or (\\\n",
    "    labels._shape.dims[0].value is None and\\\n",
    "    labels._shape.dims[1].value in [None, 10]), 'The shape of labels is incorrect'\n",
    "assert weights._variable._shape == (784, 10), 'The shape of weights is incorrect'\n",
    "assert biases._variable._shape == (10), 'The shape of biases is incorrect'\n",
    "\n",
    "assert features._dtype == tf.float32, 'features must be type float32'\n",
    "assert labels._dtype == tf.float32, 'labels must be type float32'\n",
    "\n",
    "# Feed dicts for training, validation, and test session\n",
    "train_feed_dict = {features: train_features, labels: train_labels}\n",
    "valid_feed_dict = {features: valid_features, labels: valid_labels}\n",
    "test_feed_dict = {features: test_features, labels: test_labels}\n",
    "\n",
    "# Linear Function WX + b\n",
    "logits = tf.matmul(features, weights) + biases\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy\n",
    "cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), axis=1)\n",
    "\n",
    "# some students have encountered challenges using this function, and have resolved issues\n",
    "# using https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\n",
    "# please see this thread for more detail https://discussions.udacity.com/t/accuracy-0-10-in-the-intro-to-tensorflow-lab/272469/9\n",
    "\n",
    "# Training loss\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Create an operation that initializes all variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Test Cases\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    session.run(loss, feed_dict=train_feed_dict)\n",
    "    session.run(loss, feed_dict=valid_feed_dict)\n",
    "    session.run(loss, feed_dict=test_feed_dict)\n",
    "    biases_data = session.run(biases)\n",
    "\n",
    "assert not np.count_nonzero(biases_data), 'biases must be zeros'\n",
    "\n",
    "print('Tests Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy function created.\n"
     ]
    }
   ],
   "source": [
    "# Determine if the predictions are correct\n",
    "is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "\n",
    "print('Accuracy function created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/learn_rate_tune.png\" style=\"height: 60%;width: 60%\">\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "Below are 3 parameter configurations for training the neural network. In each configuration, one of the parameters has multiple options. For each configuration, choose the option that gives the best acccuracy.\n",
    "\n",
    "Parameter configurations:\n",
    "\n",
    "Configuration 1\n",
    "* **Epochs:** 1\n",
    "* **Batch Size:**\n",
    "  * 2000\n",
    "  * 1000\n",
    "  * 500\n",
    "  * 300\n",
    "  * 50\n",
    "* **Learning Rate:** 0.01\n",
    "\n",
    "Configuration 2\n",
    "* **Epochs:** 1\n",
    "* **Batch Size:** 100\n",
    "* **Learning Rate:**\n",
    "  * 0.8\n",
    "  * 0.5\n",
    "  * 0.1\n",
    "  * 0.05\n",
    "  * 0.01\n",
    "\n",
    "Configuration 3\n",
    "* **Epochs:**\n",
    "  * 1\n",
    "  * 2\n",
    "  * 3\n",
    "  * 4\n",
    "  * 5\n",
    "* **Batch Size:** 100\n",
    "* **Learning Rate:** 0.2\n",
    "\n",
    "The code will print out a Loss and Accuracy graph, so you can see how well the neural network performed.\n",
    "\n",
    "*If you're having trouble solving problem 3, you can view the solution [here](https://github.com/udacity/CarND-TensorFlow-Lab/blob/master/solutions.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  1/4:   0%|          | 0/1425 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  1/4:   0%|          | 1/1425 [00:00<02:24,  9.84batches/s]\u001b[A\n",
      "Epoch  1/4:   4%|▎         | 51/1425 [00:00<01:38, 13.89batches/s]\u001b[A\n",
      "Epoch  1/4:   7%|▋         | 101/1425 [00:00<01:07, 19.52batches/s]\u001b[A\n",
      "Epoch  1/4:  11%|█         | 151/1425 [00:00<00:46, 27.25batches/s]\u001b[A\n",
      "Epoch  1/4:  14%|█▍        | 201/1425 [00:00<00:32, 37.71batches/s]\u001b[A\n",
      "Epoch  1/4:  18%|█▊        | 251/1425 [00:00<00:22, 51.54batches/s]\u001b[A\n",
      "Epoch  1/4:  21%|██        | 301/1425 [00:00<00:16, 69.37batches/s]\u001b[A\n",
      "Epoch  1/4:  25%|██▍       | 351/1425 [00:01<00:11, 91.40batches/s]\u001b[A\n",
      "Epoch  1/4:  28%|██▊       | 401/1425 [00:01<00:08, 117.49batches/s]\u001b[A\n",
      "Epoch  1/4:  32%|███▏      | 451/1425 [00:01<00:06, 147.17batches/s]\u001b[A\n",
      "Epoch  1/4:  35%|███▌      | 501/1425 [00:01<00:05, 178.25batches/s]\u001b[A\n",
      "Epoch  1/4:  39%|███▊      | 551/1425 [00:01<00:04, 210.27batches/s]\u001b[A\n",
      "Epoch  1/4:  42%|████▏     | 601/1425 [00:01<00:03, 241.05batches/s]\u001b[A\n",
      "Epoch  1/4:  46%|████▌     | 651/1425 [00:01<00:02, 260.91batches/s]\u001b[A\n",
      "Epoch  1/4:  49%|████▉     | 701/1425 [00:02<00:02, 271.61batches/s]\u001b[A\n",
      "Epoch  1/4:  53%|█████▎    | 751/1425 [00:02<00:02, 271.86batches/s]\u001b[A\n",
      "Epoch  1/4:  56%|█████▌    | 801/1425 [00:02<00:02, 286.01batches/s]\u001b[A\n",
      "Epoch  1/4:  60%|█████▉    | 851/1425 [00:02<00:01, 303.08batches/s]\u001b[A\n",
      "Epoch  1/4:  63%|██████▎   | 901/1425 [00:02<00:01, 312.76batches/s]\u001b[A\n",
      "Epoch  1/4:  67%|██████▋   | 951/1425 [00:02<00:01, 323.89batches/s]\u001b[A\n",
      "Epoch  1/4:  70%|███████   | 1001/1425 [00:03<00:01, 332.56batches/s]\u001b[A\n",
      "Epoch  1/4:  74%|███████▍  | 1051/1425 [00:03<00:01, 340.46batches/s]\u001b[A\n",
      "Epoch  1/4:  77%|███████▋  | 1101/1425 [00:03<00:00, 342.46batches/s]\u001b[A\n",
      "Epoch  1/4:  81%|████████  | 1151/1425 [00:03<00:00, 346.41batches/s]\u001b[A\n",
      "Epoch  1/4:  84%|████████▍ | 1201/1425 [00:03<00:00, 350.02batches/s]\u001b[A\n",
      "Epoch  1/4:  88%|████████▊ | 1251/1425 [00:03<00:00, 351.15batches/s]\u001b[A\n",
      "Epoch  1/4:  91%|█████████▏| 1301/1425 [00:03<00:00, 354.43batches/s]\u001b[A\n",
      "Epoch  1/4:  95%|█████████▍| 1351/1425 [00:03<00:00, 355.08batches/s]\u001b[A\n",
      "Epoch  1/4:  98%|█████████▊| 1401/1425 [00:04<00:00, 356.33batches/s]\u001b[A\n",
      "Epoch  1/4: 100%|██████████| 1425/1425 [00:04<00:00, 342.89batches/s]\u001b[A\n",
      "Epoch  2/4:   0%|          | 0/1425 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  2/4:   0%|          | 1/1425 [00:00<02:34,  9.23batches/s]\u001b[A\n",
      "Epoch  2/4:   4%|▎         | 51/1425 [00:00<01:45, 13.04batches/s]\u001b[A\n",
      "Epoch  2/4:   7%|▋         | 101/1425 [00:00<01:12, 18.33batches/s]\u001b[A\n",
      "Epoch  2/4:  11%|█         | 151/1425 [00:00<00:49, 25.62batches/s]\u001b[A\n",
      "Epoch  2/4:  14%|█▍        | 201/1425 [00:00<00:34, 35.51batches/s]\u001b[A\n",
      "Epoch  2/4:  18%|█▊        | 251/1425 [00:00<00:24, 48.69batches/s]\u001b[A\n",
      "Epoch  2/4:  21%|██        | 301/1425 [00:00<00:17, 65.75batches/s]\u001b[A\n",
      "Epoch  2/4:  25%|██▍       | 351/1425 [00:01<00:12, 87.24batches/s]\u001b[A\n",
      "Epoch  2/4:  28%|██▊       | 401/1425 [00:01<00:09, 112.73batches/s]\u001b[A\n",
      "Epoch  2/4:  32%|███▏      | 451/1425 [00:01<00:06, 141.90batches/s]\u001b[A\n",
      "Epoch  2/4:  35%|███▌      | 501/1425 [00:01<00:05, 173.53batches/s]\u001b[A\n",
      "Epoch  2/4:  39%|███▊      | 551/1425 [00:01<00:04, 204.22batches/s]\u001b[A\n",
      "Epoch  2/4:  42%|████▏     | 601/1425 [00:01<00:03, 223.47batches/s]\u001b[A\n",
      "Epoch  2/4:  46%|████▌     | 651/1425 [00:01<00:03, 244.71batches/s]\u001b[A\n",
      "Epoch  2/4:  49%|████▉     | 701/1425 [00:02<00:02, 270.33batches/s]\u001b[A\n",
      "Epoch  2/4:  53%|█████▎    | 751/1425 [00:02<00:02, 278.24batches/s]\u001b[A\n",
      "Epoch  2/4:  56%|█████▌    | 801/1425 [00:02<00:02, 280.88batches/s]\u001b[A\n",
      "Epoch  2/4:  60%|█████▉    | 851/1425 [00:02<00:01, 297.17batches/s]\u001b[A\n",
      "Epoch  2/4:  63%|██████▎   | 901/1425 [00:02<00:01, 312.25batches/s]\u001b[A\n",
      "Epoch  2/4:  67%|██████▋   | 951/1425 [00:02<00:01, 326.23batches/s]\u001b[A\n",
      "Epoch  2/4:  70%|███████   | 1001/1425 [00:03<00:01, 332.77batches/s]\u001b[A\n",
      "Epoch  2/4:  74%|███████▍  | 1051/1425 [00:03<00:01, 339.58batches/s]\u001b[A\n",
      "Epoch  2/4:  77%|███████▋  | 1101/1425 [00:03<00:00, 337.22batches/s]\u001b[A\n",
      "Epoch  2/4:  81%|████████  | 1151/1425 [00:03<00:00, 343.04batches/s]\u001b[A\n",
      "Epoch  2/4:  84%|████████▍ | 1201/1425 [00:03<00:00, 349.27batches/s]\u001b[A\n",
      "Epoch  2/4:  88%|████████▊ | 1251/1425 [00:03<00:00, 350.06batches/s]\u001b[A\n",
      "Epoch  2/4:  91%|█████████▏| 1301/1425 [00:03<00:00, 329.51batches/s]\u001b[A\n",
      "Epoch  2/4:  95%|█████████▍| 1351/1425 [00:04<00:00, 319.25batches/s]\u001b[A\n",
      "Epoch  2/4:  98%|█████████▊| 1401/1425 [00:04<00:00, 328.90batches/s]\u001b[A\n",
      "Epoch  2/4: 100%|██████████| 1425/1425 [00:04<00:00, 335.86batches/s]\u001b[A\n",
      "Epoch  3/4:   0%|          | 0/1425 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  3/4:   0%|          | 3/1425 [00:00<00:47, 29.95batches/s]\u001b[A\n",
      "Epoch  3/4:   4%|▎         | 51/1425 [00:00<00:33, 41.26batches/s]\u001b[A\n",
      "Epoch  3/4:   7%|▋         | 101/1425 [00:00<00:23, 56.18batches/s]\u001b[A\n",
      "Epoch  3/4:  11%|█         | 151/1425 [00:00<00:16, 75.13batches/s]\u001b[A\n",
      "Epoch  3/4:  14%|█▍        | 201/1425 [00:00<00:12, 98.83batches/s]\u001b[A\n",
      "Epoch  3/4:  18%|█▊        | 251/1425 [00:00<00:09, 125.98batches/s]\u001b[A\n",
      "Epoch  3/4:  21%|██        | 301/1425 [00:00<00:07, 156.13batches/s]\u001b[A\n",
      "Epoch  3/4:  25%|██▍       | 351/1425 [00:01<00:05, 187.67batches/s]\u001b[A\n",
      "Epoch  3/4:  28%|██▊       | 401/1425 [00:01<00:04, 218.47batches/s]\u001b[A\n",
      "Epoch  3/4:  32%|███▏      | 451/1425 [00:01<00:03, 248.03batches/s]\u001b[A\n",
      "Epoch  3/4:  35%|███▌      | 501/1425 [00:01<00:03, 272.23batches/s]\u001b[A\n",
      "Epoch  3/4:  39%|███▊      | 551/1425 [00:01<00:02, 294.01batches/s]\u001b[A\n",
      "Epoch  3/4:  42%|████▏     | 601/1425 [00:01<00:02, 309.74batches/s]\u001b[A\n",
      "Epoch  3/4:  46%|████▌     | 651/1425 [00:01<00:02, 323.72batches/s]\u001b[A\n",
      "Epoch  3/4:  49%|████▉     | 701/1425 [00:02<00:02, 333.39batches/s]\u001b[A\n",
      "Epoch  3/4:  53%|█████▎    | 751/1425 [00:02<00:01, 341.26batches/s]\u001b[A\n",
      "Epoch  3/4:  56%|█████▌    | 801/1425 [00:02<00:01, 346.27batches/s]\u001b[A\n",
      "Epoch  3/4:  60%|█████▉    | 851/1425 [00:02<00:01, 348.98batches/s]\u001b[A\n",
      "Epoch  3/4:  63%|██████▎   | 901/1425 [00:02<00:01, 352.48batches/s]\u001b[A\n",
      "Epoch  3/4:  67%|██████▋   | 951/1425 [00:02<00:01, 353.67batches/s]\u001b[A\n",
      "Epoch  3/4:  70%|███████   | 1001/1425 [00:02<00:01, 354.78batches/s]\u001b[A\n",
      "Epoch  3/4:  74%|███████▍  | 1051/1425 [00:03<00:01, 354.45batches/s]\u001b[A\n",
      "Epoch  3/4:  77%|███████▋  | 1101/1425 [00:03<00:00, 356.70batches/s]\u001b[A\n",
      "Epoch  3/4:  81%|████████  | 1151/1425 [00:03<00:00, 346.64batches/s]\u001b[A\n",
      "Epoch  3/4:  84%|████████▍ | 1201/1425 [00:03<00:00, 330.68batches/s]\u001b[A\n",
      "Epoch  3/4:  88%|████████▊ | 1251/1425 [00:03<00:00, 338.61batches/s]\u001b[A\n",
      "Epoch  3/4:  91%|█████████▏| 1301/1425 [00:03<00:00, 342.77batches/s]\u001b[A\n",
      "Epoch  3/4:  95%|█████████▍| 1351/1425 [00:03<00:00, 346.27batches/s]\u001b[A\n",
      "Epoch  3/4:  98%|█████████▊| 1401/1425 [00:04<00:00, 348.78batches/s]\u001b[A\n",
      "Epoch  3/4: 100%|██████████| 1425/1425 [00:04<00:00, 349.68batches/s]\u001b[A\n",
      "Epoch  4/4:   0%|          | 0/1425 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  4/4:   0%|          | 4/1425 [00:00<00:35, 39.75batches/s]\u001b[A\n",
      "Epoch  4/4:   4%|▎         | 51/1425 [00:00<00:25, 54.14batches/s]\u001b[A\n",
      "Epoch  4/4:   7%|▋         | 101/1425 [00:00<00:18, 72.75batches/s]\u001b[A\n",
      "Epoch  4/4:  11%|█         | 151/1425 [00:00<00:13, 95.63batches/s]\u001b[A\n",
      "Epoch  4/4:  14%|█▍        | 201/1425 [00:00<00:09, 122.53batches/s]\u001b[A\n",
      "Epoch  4/4:  18%|█▊        | 251/1425 [00:00<00:07, 152.59batches/s]\u001b[A\n",
      "Epoch  4/4:  21%|██        | 301/1425 [00:00<00:06, 184.56batches/s]\u001b[A\n",
      "Epoch  4/4:  25%|██▍       | 351/1425 [00:01<00:04, 216.79batches/s]\u001b[A\n",
      "Epoch  4/4:  28%|██▊       | 401/1425 [00:01<00:04, 246.84batches/s]\u001b[A\n",
      "Epoch  4/4:  32%|███▏      | 451/1425 [00:01<00:03, 272.52batches/s]\u001b[A\n",
      "Epoch  4/4:  35%|███▌      | 501/1425 [00:01<00:03, 293.35batches/s]\u001b[A\n",
      "Epoch  4/4:  39%|███▊      | 551/1425 [00:01<00:02, 302.47batches/s]\u001b[A\n",
      "Epoch  4/4:  42%|████▏     | 601/1425 [00:01<00:02, 297.98batches/s]\u001b[A\n",
      "Epoch  4/4:  46%|████▌     | 651/1425 [00:01<00:02, 300.97batches/s]\u001b[A\n",
      "Epoch  4/4:  49%|████▉     | 701/1425 [00:02<00:02, 316.02batches/s]\u001b[A\n",
      "Epoch  4/4:  53%|█████▎    | 751/1425 [00:02<00:02, 323.53batches/s]\u001b[A\n",
      "Epoch  4/4:  56%|█████▌    | 801/1425 [00:02<00:01, 318.29batches/s]\u001b[A\n",
      "Epoch  4/4:  60%|█████▉    | 851/1425 [00:02<00:01, 306.44batches/s]\u001b[A\n",
      "Epoch  4/4:  63%|██████▎   | 901/1425 [00:02<00:01, 312.47batches/s]\u001b[A\n",
      "Epoch  4/4:  67%|██████▋   | 951/1425 [00:02<00:01, 322.03batches/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4/4:  70%|███████   | 1001/1425 [00:03<00:01, 329.62batches/s]\u001b[A\n",
      "Epoch  4/4:  74%|███████▍  | 1051/1425 [00:03<00:01, 318.00batches/s]\u001b[A\n",
      "Epoch  4/4:  77%|███████▋  | 1101/1425 [00:03<00:01, 314.33batches/s]\u001b[A\n",
      "Epoch  4/4:  81%|████████  | 1151/1425 [00:03<00:00, 307.44batches/s]\u001b[A\n",
      "Epoch  4/4:  84%|████████▍ | 1201/1425 [00:03<00:00, 296.95batches/s]\u001b[A\n",
      "Epoch  4/4:  88%|████████▊ | 1251/1425 [00:03<00:00, 297.49batches/s]\u001b[A\n",
      "Epoch  4/4:  91%|█████████▏| 1301/1425 [00:04<00:00, 292.49batches/s]\u001b[A\n",
      "Epoch  4/4:  95%|█████████▍| 1351/1425 [00:04<00:00, 291.72batches/s]\u001b[A\n",
      "Epoch  4/4:  98%|█████████▊| 1401/1425 [00:04<00:00, 300.18batches/s]\u001b[A\n",
      "Epoch  4/4: 100%|██████████| 1425/1425 [00:04<00:00, 322.66batches/s]\u001b[A"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc74d12f6d8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at 0.7868000268936157\n"
     ]
    }
   ],
   "source": [
    "# TODO: Find the best parameters for each configuration\n",
    "epochs = 4\n",
    "batch_size = 100\n",
    "learning_rate = 0.2\n",
    "\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    \n",
    "\n",
    "# The accuracy measured against the validation set\n",
    "validation_accuracy = 0.0\n",
    "\n",
    "# Measurements use for graphing loss and accuracy\n",
    "log_batch_step = 50\n",
    "batches = []\n",
    "loss_batch = []\n",
    "train_acc_batch = []\n",
    "valid_acc_batch = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        # Progress bar\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "        \n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "\n",
    "            # Run optimizer and get loss\n",
    "            _, l = session.run(\n",
    "                [optimizer, loss],\n",
    "                feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "            # Log every 50 batches\n",
    "            if not batch_i % log_batch_step:\n",
    "                # Calculate Training and Validation accuracy\n",
    "                training_accuracy = session.run(accuracy, feed_dict=train_feed_dict)\n",
    "                validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
    "\n",
    "                # Log batches\n",
    "                previous_batch = batches[-1] if batches else 0\n",
    "                batches.append(log_batch_step + previous_batch)\n",
    "                loss_batch.append(l)\n",
    "                train_acc_batch.append(training_accuracy)\n",
    "                valid_acc_batch.append(validation_accuracy)\n",
    "\n",
    "        # Check accuracy against Validation data\n",
    "        validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
    "\n",
    "loss_plot = plt.subplot(211)\n",
    "loss_plot.set_title('Loss')\n",
    "loss_plot.plot(batches, loss_batch, 'g')\n",
    "loss_plot.set_xlim([batches[0], batches[-1]])\n",
    "acc_plot = plt.subplot(212)\n",
    "acc_plot.set_title('Accuracy')\n",
    "acc_plot.plot(batches, train_acc_batch, 'r', label='Training Accuracy')\n",
    "acc_plot.plot(batches, valid_acc_batch, 'x', label='Validation Accuracy')\n",
    "acc_plot.set_ylim([0, 1.0])\n",
    "acc_plot.set_xlim([batches[0], batches[-1]])\n",
    "acc_plot.legend(loc=4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Validation accuracy at {}'.format(validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Set the epochs, batch_size, and learning_rate with the best learning parameters you discovered in problem 3.  You're going to test your model against your hold out dataset/testing data.  This will give you a good indicator of how well the model will do in the real world.  You should have a test accuracy of at least 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  1/4:   0%|          | 0/1425 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  1/4:   8%|▊         | 120/1425 [00:00<00:01, 1195.05batches/s]\u001b[A\n",
      "Epoch  1/4:  15%|█▌        | 219/1425 [00:00<00:01, 1122.99batches/s]\u001b[A\n",
      "Epoch  1/4:  23%|██▎       | 324/1425 [00:00<00:01, 1099.45batches/s]\u001b[A\n",
      "Epoch  1/4:  30%|███       | 428/1425 [00:00<00:00, 1079.70batches/s]\u001b[A\n",
      "Epoch  1/4:  37%|███▋      | 530/1425 [00:00<00:00, 1059.81batches/s]\u001b[A\n",
      "Epoch  1/4:  44%|████▍     | 633/1425 [00:00<00:00, 1048.97batches/s]\u001b[A\n",
      "Epoch  1/4:  52%|█████▏    | 735/1425 [00:00<00:00, 1040.06batches/s]\u001b[A\n",
      "Epoch  1/4:  59%|█████▊    | 834/1425 [00:00<00:00, 1024.26batches/s]\u001b[A\n",
      "Epoch  1/4:  65%|██████▌   | 932/1425 [00:00<00:00, 1009.41batches/s]\u001b[A\n",
      "Epoch  1/4:  74%|███████▍  | 1054/1425 [00:01<00:00, 1062.50batches/s]\u001b[A\n",
      "Epoch  1/4:  81%|████████▏ | 1158/1425 [00:01<00:00, 1049.66batches/s]\u001b[A\n",
      "Epoch  1/4:  89%|████████▊ | 1262/1425 [00:01<00:00, 1012.30batches/s]\u001b[A\n",
      "Epoch  1/4:  96%|█████████▌| 1364/1425 [00:01<00:00, 1014.13batches/s]\u001b[A\n",
      "Epoch  1/4: 100%|██████████| 1425/1425 [00:01<00:00, 1035.59batches/s]\u001b[A\n",
      "Epoch  2/4:   0%|          | 0/1425 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  2/4:   8%|▊         | 113/1425 [00:00<00:01, 1120.60batches/s]\u001b[A\n",
      "Epoch  2/4:  16%|█▌        | 230/1425 [00:00<00:01, 1133.44batches/s]\u001b[A\n",
      "Epoch  2/4:  24%|██▎       | 338/1425 [00:00<00:00, 1115.86batches/s]\u001b[A\n",
      "Epoch  2/4:  31%|███▏      | 446/1425 [00:00<00:00, 1103.45batches/s]\u001b[A\n",
      "Epoch  2/4:  39%|███▉      | 556/1425 [00:00<00:00, 1102.02batches/s]\u001b[A\n",
      "Epoch  2/4:  47%|████▋     | 664/1425 [00:00<00:00, 1093.62batches/s]\u001b[A\n",
      "Epoch  2/4:  54%|█████▎    | 765/1425 [00:00<00:00, 1066.06batches/s]\u001b[A\n",
      "Epoch  2/4:  61%|██████    | 868/1425 [00:00<00:00, 1052.14batches/s]\u001b[A\n",
      "Epoch  2/4:  68%|██████▊   | 975/1425 [00:00<00:00, 1055.95batches/s]\u001b[A\n",
      "Epoch  2/4:  78%|███████▊  | 1112/1425 [00:01<00:00, 1132.82batches/s]\u001b[A\n",
      "Epoch  2/4:  86%|████████▌ | 1224/1425 [00:01<00:00, 1120.41batches/s]\u001b[A\n",
      "Epoch  2/4:  94%|█████████▍| 1336/1425 [00:01<00:00, 1096.43batches/s]\u001b[A\n",
      "Epoch  2/4: 100%|██████████| 1425/1425 [00:01<00:00, 1090.30batches/s]\u001b[A\n",
      "Epoch  3/4:   0%|          | 0/1425 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  3/4:   7%|▋         | 106/1425 [00:00<00:01, 1050.71batches/s]\u001b[A\n",
      "Epoch  3/4:  15%|█▍        | 211/1425 [00:00<00:01, 1048.22batches/s]\u001b[A\n",
      "Epoch  3/4:  23%|██▎       | 321/1425 [00:00<00:01, 1062.19batches/s]\u001b[A\n",
      "Epoch  3/4:  30%|███       | 428/1425 [00:00<00:00, 1063.75batches/s]\u001b[A\n",
      "Epoch  3/4:  37%|███▋      | 530/1425 [00:00<00:00, 1047.28batches/s]\u001b[A\n",
      "Epoch  3/4:  45%|████▍     | 637/1425 [00:00<00:00, 1051.27batches/s]\u001b[A\n",
      "Epoch  3/4:  52%|█████▏    | 734/1425 [00:00<00:00, 1022.59batches/s]\u001b[A\n",
      "Epoch  3/4:  59%|█████▉    | 839/1425 [00:00<00:00, 1030.26batches/s]\u001b[A\n",
      "Epoch  3/4:  66%|██████▋   | 947/1425 [00:00<00:00, 1041.40batches/s]\u001b[A\n",
      "Epoch  3/4:  74%|███████▍  | 1055/1425 [00:01<00:00, 1052.52batches/s]\u001b[A\n",
      "Epoch  3/4:  81%|████████▏ | 1160/1425 [00:01<00:00, 1049.88batches/s]\u001b[A\n",
      "Epoch  3/4:  90%|████████▉ | 1281/1425 [00:01<00:00, 1091.12batches/s]\u001b[A\n",
      "Epoch  3/4:  99%|█████████▉| 1416/1425 [00:01<00:00, 1156.83batches/s]\u001b[A\n",
      "Epoch  3/4: 100%|██████████| 1425/1425 [00:01<00:00, 1082.41batches/s]\u001b[A\n",
      "Epoch  4/4:   0%|          | 0/1425 [00:00<?, ?batches/s]\u001b[A\n",
      "Epoch  4/4:   8%|▊         | 113/1425 [00:00<00:01, 1120.91batches/s]\u001b[A\n",
      "Epoch  4/4:  15%|█▌        | 220/1425 [00:00<00:01, 1104.39batches/s]\u001b[A\n",
      "Epoch  4/4:  23%|██▎       | 331/1425 [00:00<00:00, 1103.73batches/s]\u001b[A\n",
      "Epoch  4/4:  31%|███       | 442/1425 [00:00<00:00, 1105.40batches/s]\u001b[A\n",
      "Epoch  4/4:  38%|███▊      | 536/1425 [00:00<00:00, 1049.88batches/s]\u001b[A\n",
      "Epoch  4/4:  45%|████▍     | 636/1425 [00:00<00:00, 1034.04batches/s]\u001b[A\n",
      "Epoch  4/4:  52%|█████▏    | 734/1425 [00:00<00:00, 1016.70batches/s]\u001b[A\n",
      "Epoch  4/4:  59%|█████▊    | 837/1425 [00:00<00:00, 1020.51batches/s]\u001b[A\n",
      "Epoch  4/4:  66%|██████▌   | 943/1425 [00:00<00:00, 1031.58batches/s]\u001b[A\n",
      "Epoch  4/4:  74%|███████▍  | 1051/1425 [00:01<00:00, 1044.27batches/s]\u001b[A\n",
      "Epoch  4/4:  81%|████████  | 1157/1425 [00:01<00:00, 1046.42batches/s]\u001b[A\n",
      "Epoch  4/4:  89%|████████▉ | 1266/1425 [00:01<00:00, 1058.12batches/s]\u001b[A\n",
      "Epoch  4/4:  96%|█████████▌| 1371/1425 [00:01<00:00, 1036.46batches/s]\u001b[A\n",
      "Epoch  4/4: 100%|██████████| 1425/1425 [00:01<00:00, 1046.88batches/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice Job! Test Accuracy is 0.8553000092506409\n"
     ]
    }
   ],
   "source": [
    "# TODO: Set the epochs, batch_size, and learning_rate with the best parameters from problem 3\n",
    "epochs = 4\n",
    "batch_size = 100\n",
    "learning_rate = 0.2\n",
    "\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# The accuracy measured against the test set\n",
    "test_accuracy = 0.0\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    session.run(init)\n",
    "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        # Progress bar\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "        \n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "\n",
    "            # Run optimizer\n",
    "            _ = session.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "        # Check accuracy against Test data\n",
    "        test_accuracy = session.run(accuracy, feed_dict=test_feed_dict)\n",
    "\n",
    "\n",
    "assert test_accuracy >= 0.80, 'Test accuracy at {}, should be equal to or greater than 0.80'.format(test_accuracy)\n",
    "print('Nice Job! Test Accuracy is {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple layers\n",
    "\n",
    "Good job!  You built a one layer TensorFlow network!  However, you want to build more than one layer.  This is deep learning after all!  In the next section, you will start to satisfy your need for more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
